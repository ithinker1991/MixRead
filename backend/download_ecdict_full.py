#!/usr/bin/env python3
"""
ä¸‹è½½å®Œæ•´ ECDICT å¹¶æå– CEFR è¯æ±‡ç¿»è¯‘
ä½¿ç”¨æµå¼å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
"""

import json
import csv
from pathlib import Path
import requests  # ä½¿ç”¨ requests æ›¿ä»£ httpx ä»¥è·å¾—æ›´å¥½çš„æµå¼æ”¯æŒ

def download_ecdict_full():
    """
    æµå¼ä¸‹è½½å®Œæ•´ ECDICT
    """

    print("=" * 70)
    print("ğŸ¯ ä¸‹è½½å®Œæ•´ ECDICT (çº¦ 770K è¯æ¡)")
    print("=" * 70)

    # Load CEFR words
    cefr_path = Path(__file__).parent / "data" / "cefr_words.json"
    with open(cefr_path, 'r', encoding='utf-8') as f:
        cefr_data = json.load(f)

    cefr_words = set(word.lower() for word in cefr_data.keys())
    print(f"\nğŸ“š CEFR è¯åº“: {len(cefr_words)} ä¸ªå•è¯")

    # Full ECDICT URL
    full_url = "https://raw.githubusercontent.com/skywind3000/ECDICT/master/ecdict.csv"

    print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½å®Œæ•´ ECDICT...")
    print(f"   æ¥æº: {full_url}")
    print(f"   â±  é¢„è®¡æ—¶é—´: 2-5 åˆ†é’Ÿï¼ˆå–å†³äºç½‘é€Ÿï¼‰")
    print(f"   ğŸ’¡ ä½¿ç”¨æµå¼å¤„ç†ï¼Œä¸ä¼šå ç”¨å¤ªå¤šå†…å­˜")

    translations = {}
    processed = 0
    matched = 0

    try:
        print("\nğŸ”„ ä¸‹è½½å¹¶å¤„ç†ä¸­ï¼ˆè¯·è€å¿ƒç­‰å¾…ï¼‰...\n")

        # Stream download
        with requests.get(full_url, stream=True, timeout=300) as response:
            response.raise_for_status()

            # Decode and process line by line
            lines = response.iter_lines(decode_unicode=True)

            # Get header
            header_line = next(lines)
            fieldnames = header_line.split(',')

            # Find indices for columns we need
            word_idx = fieldnames.index('word')
            trans_idx = fieldnames.index('translation')

            # Process each line
            for line in lines:
                if not line.strip():
                    continue

                processed += 1

                # Progress indicator
                if processed % 10000 == 0:
                    print(f"   âœ“ å¤„ç†äº† {processed:,} æ¡ï¼Œæ‰¾åˆ° {matched}/{len(cefr_words)} ä¸ªåŒ¹é… ({matched/len(cefr_words)*100:.1f}%)")

                try:
                    # Simple CSV parsing (handle basic cases)
                    parts = line.split(',')
                    if len(parts) < max(word_idx, trans_idx) + 1:
                        continue

                    word = parts[word_idx].strip().strip('"').lower()
                    translation = ','.join(parts[trans_idx:]).strip().strip('"')

                    # Only process CEFR words
                    if word in cefr_words and translation:
                        chinese = extract_chinese(translation)
                        if chinese and word not in translations:  # Don't overwrite
                            translations[word] = chinese
                            matched += 1

                            # Early exit if we found most words
                            if matched >= len(cefr_words) * 0.98:  # 98% is excellent
                                print(f"\n   ğŸ‰ è¾¾åˆ° 98% è¦†ç›–ç‡ï¼Œæå‰ç»“æŸ...")
                                break

                except Exception as e:
                    # Skip problematic lines
                    continue

        print(f"\nâœ… ä¸‹è½½å¹¶å¤„ç†å®Œæˆ!")
        print(f"   æ€»å…±å¤„ç†: {processed:,} æ¡è®°å½•")
        print(f"   åŒ¹é…æˆåŠŸ: {matched} ä¸ª CEFR å•è¯")
        print(f"   è¦†ç›–ç‡: {matched/len(cefr_words)*100:.1f}%")

    except requests.exceptions.RequestException as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        print("\nğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("   - ç½‘ç»œè¿æ¥é—®é¢˜")
        print("   - GitHub è®¿é—®å—é™")
        print("\nå»ºè®®:")
        print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   2. ä½¿ç”¨ VPN æˆ–é•œåƒ")
        print("   3. æˆ–ä½¿ç”¨å½“å‰ 1708 è¯è¯å…¸")
        return None

    # Merge with existing dictionary
    existing_dict_file = Path(__file__).parent / "chinese_dict.json"
    try:
        with open(existing_dict_file, 'r', encoding='utf-8') as f:
            existing_dict = json.load(f)
        print(f"\nğŸ“– å½“å‰è¯å…¸: {len(existing_dict)} è¯")

        # Merge: prefer existing manual translations
        added = 0
        for word, translation in translations.items():
            if word not in existing_dict:
                existing_dict[word] = translation
                added += 1

        translations = existing_dict
        print(f"   æ–°å¢: {added} è¯")
        print(f"   åˆå¹¶å: {len(translations)} è¯")

    except FileNotFoundError:
        pass

    # Save to file
    output_file = Path(__file__).parent / "chinese_dict.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(translations, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   CEFR è¯åº“: {len(cefr_words)} è¯")
    print(f"   ä¸­æ–‡è¯å…¸: {len(translations)} è¯")
    print(f"   è¦†ç›–ç‡: {len(translations)/len(cefr_words)*100:.1f}%")

    # Show coverage by level
    show_coverage_by_level(translations, cefr_data)

    print(f"\nğŸ‰ å®Œæˆï¼ç°åœ¨ä½ çš„è¯å…¸è¦†ç›–ç‡å¤§å¹…æå‡ï¼")
    print(f"   å»ºè®®ï¼šé‡å¯åç«¯ä»¥åŠ è½½æ–°è¯å…¸")

    return output_file


def extract_chinese(translation: str) -> str:
    """
    ä» ECDICT ç¿»è¯‘å­—æ®µä¸­æå–ä¸­æ–‡
    ä¼˜åŒ–ï¼šåªä¿ç•™ç¬¬ä¸€ä¸ªã€æœ€ç®€æ´çš„é‡Šä¹‰
    """
    if not translation:
        return ""

    # First, remove any CSV artifacts (trailing commas and fields)
    translation = translation.split(',,')[0].strip()

    chinese_parts = []

    # Split by common separators to get multiple meanings
    for separator in ['\\n', '\n', 'ï¼›', ';']:
        if separator in translation:
            translation = translation.replace(separator, '|')

    for part in translation.split('|'):
        cleaned = part.strip()

        # Remove English POS markers (n., v., adj., etc.)
        for prefix in ['n.', 'v.', 'vt.', 'vi.', 'adj.', 'adv.', 'prep.',
                       'conj.', 'pron.', 'interj.', 'abbr.', 'num.', 'art.',
                       'a.', 'aux.', 'det.', 'modal.', 'inf.']:
            if cleaned.lower().startswith(prefix):
                cleaned = cleaned[len(prefix):].strip()
                break

        # Check if starts with Chinese character
        if cleaned and len(cleaned) > 0:
            first_char = cleaned[0]
            # Check if Chinese (CJK Unified Ideographs range)
            if '\u4e00' <= first_char <= '\u9fff':
                # Keep only the first meaning (before comma)
                cleaned = cleaned.split(',')[0].strip()
                cleaned = cleaned.split('ï¼Œ')[0].strip()

                # Remove parentheses and brackets content
                cleaned = cleaned.split('(')[0].strip()
                cleaned = cleaned.split('ï¼ˆ')[0].strip()
                cleaned = cleaned.split('[')[0].strip()

                if cleaned and len(cleaned) <= 8:  # Prefer concise translations
                    chinese_parts.append(cleaned)

    # Return first valid Chinese translation
    return chinese_parts[0] if chinese_parts else ""


def show_coverage_by_level(translations: dict, cefr_data: dict):
    """æ˜¾ç¤ºå„éš¾åº¦çº§åˆ«çš„è¦†ç›–ç‡"""
    levels = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']

    print(f"\nğŸ“ˆ å„çº§åˆ«è¦†ç›–ç‡:")
    for level in levels:
        level_words = [w for w in cefr_data if cefr_data[w].get('cefr_level') == level]
        if not level_words:
            continue

        covered = sum(1 for w in level_words if w.lower() in translations)
        coverage = covered / len(level_words) * 100 if level_words else 0

        bar_length = int(coverage / 2)  # Scale to 50 chars max
        bar = 'â–ˆ' * bar_length + 'â–‘' * (50 - bar_length)

        print(f"   {level}: {bar} {covered}/{len(level_words)} ({coverage:.1f}%)")


if __name__ == "__main__":
    download_ecdict_full()

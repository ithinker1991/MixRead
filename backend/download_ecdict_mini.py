#!/usr/bin/env python3
"""
ä¸‹è½½ ECDICT Mini ç‰ˆæœ¬ï¼ˆæ›´å°ï¼Œæ›´å¿«ï¼‰
ç„¶åæå– CEFR è¯æ±‡çš„ä¸­æ–‡ç¿»è¯‘
"""

import json
import csv
import httpx
from pathlib import Path
from io import StringIO

def download_ecdict_mini():
    """
    ä¸‹è½½ ECDICT Mini ç‰ˆæœ¬å¹¶æå– CEFR è¯æ±‡ç¿»è¯‘
    """

    print("=" * 70)
    print("ğŸ¯ ä¸‹è½½ ECDICT Mini ç‰ˆæœ¬")
    print("=" * 70)

    # Load CEFR words
    cefr_path = Path(__file__).parent / "data" / "cefr_words.json"
    with open(cefr_path, 'r', encoding='utf-8') as f:
        cefr_data = json.load(f)

    cefr_words = set(word.lower() for word in cefr_data.keys())
    print(f"\nğŸ“š CEFR è¯åº“: {len(cefr_words)} ä¸ªå•è¯")

    # ECDICT Mini URL (smaller file, faster download)
    mini_url = "https://raw.githubusercontent.com/skywind3000/ECDICT/master/ecdict.mini.csv"

    print(f"\nğŸ“¥ ä¸‹è½½ ECDICT Mini...")
    print(f"   æ¥æº: {mini_url}")
    print(f"   â±  é¢„è®¡æ—¶é—´: 30-60 ç§’")

    translations = {}
    processed = 0
    matched = 0

    try:
        print("\nğŸ”„ ä¸‹è½½ä¸­...")

        # Download the file
        with httpx.Client(timeout=120.0) as client:
            response = client.get(mini_url, follow_redirects=True)
            response.raise_for_status()

            print("âœ… ä¸‹è½½å®Œæˆï¼Œå¼€å§‹å¤„ç†...")

            # Parse CSV
            content = response.text
            csv_reader = csv.DictReader(StringIO(content), delimiter=',')

            for row in csv_reader:
                processed += 1

                if processed % 5000 == 0:
                    print(f"   å¤„ç†äº† {processed:,} æ¡ï¼Œæ‰¾åˆ° {matched} ä¸ªåŒ¹é…...")

                word = row.get('word', '').lower().strip()

                # Only process CEFR words
                if word in cefr_words:
                    translation = row.get('translation', '').strip()

                    if translation:
                        # Clean up translation
                        chinese = extract_chinese(translation)
                        if chinese:
                            translations[word] = chinese
                            matched += 1

        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"   æ€»å…±å¤„ç†: {processed:,} æ¡è®°å½•")
        print(f"   åŒ¹é…æˆåŠŸ: {matched} ä¸ª CEFR å•è¯")
        print(f"   è¦†ç›–ç‡: {matched/len(cefr_words)*100:.1f}%")

    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        print("\nä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ...")
        return use_stardict_format()

    # Merge with existing dictionary (keep manual entries)
    existing_dict_file = Path(__file__).parent / "chinese_dict.json"
    try:
        with open(existing_dict_file, 'r', encoding='utf-8') as f:
            existing_dict = json.load(f)
        print(f"\nğŸ“– å½“å‰è¯å…¸: {len(existing_dict)} è¯")

        # Merge: prefer existing manual translations
        for word, translation in translations.items():
            if word not in existing_dict:
                existing_dict[word] = translation

        translations = existing_dict
        print(f"   åˆå¹¶å: {len(translations)} è¯")

    except FileNotFoundError:
        pass

    # Save to file
    output_file = Path(__file__).parent / "chinese_dict.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(translations, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   CEFR è¯åº“: {len(cefr_words)} è¯")
    print(f"   ä¸­æ–‡è¯å…¸: {len(translations)} è¯")
    print(f"   è¦†ç›–ç‡: {len(translations)/len(cefr_words)*100:.1f}%")

    # Show coverage by level
    show_coverage_by_level(translations, cefr_data)

    return output_file


def extract_chinese(translation: str) -> str:
    """
    ä» ECDICT ç¿»è¯‘å­—æ®µä¸­æå–ä¸­æ–‡
    æ ¼å¼ä¾‹å­: "n. ä¹¦\\nvt. é¢„è®¢\\nvi. è®¢ç¥¨"
    """
    if not translation:
        return ""

    chinese_parts = []
    for part in translation.split('\\n'):
        # Remove English POS markers
        cleaned = part.strip()
        for prefix in ['n.', 'v.', 'vt.', 'vi.', 'adj.', 'adv.', 'prep.',
                       'conj.', 'pron.', 'interj.', 'abbr.', 'num.', 'art.']:
            if cleaned.startswith(prefix):
                cleaned = cleaned[len(prefix):].strip()
                break

        # Check if starts with Chinese character
        if cleaned and ord(cleaned[0]) > 127:  # Non-ASCII (likely Chinese)
            chinese_parts.append(cleaned)

    # Return first valid Chinese translation
    return chinese_parts[0] if chinese_parts else ""


def show_coverage_by_level(translations: dict, cefr_data: dict):
    """æ˜¾ç¤ºå„éš¾åº¦çº§åˆ«çš„è¦†ç›–ç‡"""
    levels = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']

    print(f"\nğŸ“ˆ å„çº§åˆ«è¦†ç›–ç‡:")
    for level in levels:
        level_words = [w for w in cefr_data if cefr_data[w].get('cefr_level') == level]
        if not level_words:
            continue

        covered = sum(1 for w in level_words if w.lower() in translations)
        coverage = covered / len(level_words) * 100 if level_words else 0

        print(f"   {level}: {covered}/{len(level_words)} ({coverage:.1f}%)")


def use_stardict_format():
    """å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ StarDict æ ¼å¼æˆ–å…¶ä»–æ¥æº"""
    print("\n=" * 70)
    print("ğŸ“Œ å¤‡é€‰æ–¹æ¡ˆ")
    print("=" * 70)
    print("\næ¨èä½¿ç”¨ç¿»è¯‘ API:")
    print("1. æœ‰é“æ™ºäº‘ API (å…è´¹é¢åº¦ 100æ¬¡/å¤©)")
    print("2. ç™¾åº¦ç¿»è¯‘ API (æ ‡å‡†ç‰ˆå…è´¹)")
    print("\næˆ–è€…ä½¿ç”¨å½“å‰çš„ 1708 è¯è¯å…¸ç»§ç»­ã€‚")
    return None


if __name__ == "__main__":
    download_ecdict_mini()

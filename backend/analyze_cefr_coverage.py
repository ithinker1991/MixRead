#!/usr/bin/env python3
"""
åˆ†æ CEFR æ•°æ®åº“ä¸­æœ‰å¤šå°‘è¯ç¼ºå°‘ä¸­æ–‡ç¿»è¯‘
Analyze how many CEFR words are missing Chinese translations
"""

import json
from pathlib import Path

# Load CEFR database
cefr_path = Path(__file__).parent / "data" / "cefr_words.json"
with open(cefr_path, 'r', encoding='utf-8') as f:
    cefr_data = json.load(f)

# Load Chinese dictionary
chinese_path = Path(__file__).parent / "chinese_dict.json"
with open(chinese_path, 'r', encoding='utf-8') as f:
    chinese_dict = json.load(f)

print("ğŸ“Š CEFR è¯åº“ä¸ä¸­æ–‡è¯å…¸è¦†ç›–ç‡åˆ†æ")
print("=" * 60)

# Count by CEFR level
levels = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
total_words = 0
total_with_chinese = 0

level_stats = {}

for level in levels:
    level_words = [w for w in cefr_data if cefr_data[w].get('cefr_level') == level]
    words_with_chinese = [w for w in level_words if w.lower() in chinese_dict]

    level_stats[level] = {
        'total': len(level_words),
        'with_chinese': len(words_with_chinese),
        'coverage': len(words_with_chinese) / len(level_words) * 100 if level_words else 0,
        'missing': [w for w in level_words[:20] if w.lower() not in chinese_dict]  # Sample
    }

    total_words += len(level_words)
    total_with_chinese += len(words_with_chinese)

    print(f"\n{level} çº§åˆ«:")
    print(f"  æ€»è¯æ•°: {len(level_words)}")
    print(f"  æœ‰ä¸­æ–‡: {len(words_with_chinese)} ({level_stats[level]['coverage']:.1f}%)")
    print(f"  ç¼ºå°‘ä¸­æ–‡: {len(level_words) - len(words_with_chinese)}")
    if level_stats[level]['missing']:
        print(f"  ç¤ºä¾‹ç¼ºå¤±è¯: {', '.join(level_stats[level]['missing'][:10])}")

print(f"\n{'='*60}")
print(f"æ€»è®¡:")
print(f"  CEFR æ€»è¯æ•°: {total_words}")
print(f"  æœ‰ä¸­æ–‡: {total_with_chinese} ({total_with_chinese/total_words*100:.1f}%)")
print(f"  ç¼ºå°‘ä¸­æ–‡: {total_words - total_with_chinese}")

print(f"\n{'='*60}")
print("ğŸ’¡ å»ºè®® Recommendations:")
print()
print("1. æœ€ç®€å•ï¼šæ‰©å……ä¸­æ–‡è¯å…¸åˆ°å¸¸ç”¨çš„ 1000-2000 è¯")
print("2. å®Œæ•´è¦†ç›–ï¼šæ·»åŠ æ‰€æœ‰ CEFR è¯çš„ç¿»è¯‘ (6860 è¯)")
print("3. æ™ºèƒ½æ–¹æ¡ˆï¼šä½¿ç”¨ç¿»è¯‘ API ä½œä¸ºå¤‡é€‰")
print("4. æ··åˆæ–¹æ¡ˆï¼šA1-B1 å®Œæ•´è¦†ç›–ï¼ŒB2-C2 ä½¿ç”¨ API å¤‡é€‰")

# Find most common missing words in B1 and below (should prioritize these)
common_levels = ['A1', 'A2', 'B1']
common_missing = []
for level in common_levels:
    level_words = [w for w in cefr_data if cefr_data[w].get('cefr_level') == level]
    missing = [w.lower() for w in level_words if w.lower() not in chinese_dict]
    common_missing.extend(missing)

print(f"\n{'='*60}")
print(f"ğŸ¯ é«˜ä¼˜å…ˆçº§ç¼ºå¤±è¯æ±‡ (A1-B1):")
print(f"   æ€»æ•°: {len(common_missing)} ä¸ª")
if common_missing:
    print(f"   å‰ 50 ä¸ª: {', '.join(sorted(set(common_missing))[:50])}")

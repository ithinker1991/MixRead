#!/usr/bin/env python3
"""
æ¸…ç†ä¸­æ–‡ç¿»è¯‘ï¼Œåªä¿ç•™æœ€æ ¸å¿ƒçš„é‡Šä¹‰
- å»é™¤å¤šä½™é‡Šä¹‰ï¼ˆåªä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
- å»é™¤è„æ•°æ®
- é™åˆ¶é•¿åº¦ï¼ˆæœ€å¤š2ä¸ªè¯ï¼‰
"""

import json
import re
from pathlib import Path

def clean_translation(text: str) -> str:
    """
    æ¸…ç†ç¿»è¯‘æ–‡æœ¬ï¼Œè¿”å›æœ€ç®€æ´çš„ç‰ˆæœ¬
    """
    if not text:
        return ""

    # Remove CSV artifacts (anything after ",, or "",)
    text = re.split(r'[",]{2,}', text)[0]

    # Remove extra whitespace
    text = text.strip()

    # Split by common separators
    for sep in [',', 'ï¼Œ', ';', 'ï¼›', 'ã€']:
        if sep in text:
            parts = [p.strip() for p in text.split(sep) if p.strip()]
            if parts:
                text = parts[0]  # Take first meaning
                break

    # Limit length: prefer shorter translations
    # If too long (>6 chars), try to find a shorter alternative
    if len(text) > 6:
        # Check if there are parentheses or brackets
        text = re.sub(r'\([^)]*\)', '', text)  # Remove (...)
        text = re.sub(r'\[[^\]]*\]', '', text)  # Remove [...]
        text = text.strip()

    # Remove any remaining non-Chinese characters at the end
    text = re.sub(r'[^\\u4e00-\\u9fa5]+$', '', text)

    return text.strip()


def clean_dictionary():
    """
    æ¸…ç†æ•´ä¸ªè¯å…¸
    """
    dict_path = Path(__file__).parent / "chinese_dict.json"

    print("=" * 70)
    print("ğŸ§¹ æ¸…ç†ä¸­æ–‡è¯å…¸")
    print("=" * 70)

    # Load dictionary
    with open(dict_path, 'r', encoding='utf-8') as f:
        original_dict = json.load(f)

    print(f"\nğŸ“š åŸè¯å…¸: {len(original_dict)} è¯")

    # Clean each entry
    cleaned_dict = {}
    problematic = []
    cleaned_count = 0

    for word, translation in original_dict.items():
        cleaned = clean_translation(translation)

        if not cleaned:
            problematic.append(word)
            continue

        if cleaned != translation:
            cleaned_count += 1
            if cleaned_count <= 10:  # Show first 10 examples
                print(f"\næ¸…ç†ç¤ºä¾‹:")
                print(f"  {word}")
                print(f"    åŸæ–‡: {translation[:50]}{'...' if len(translation) > 50 else ''}")
                print(f"    æ¸…ç†: {cleaned}")

        cleaned_dict[word] = cleaned

    print(f"\nâœ… æ¸…ç†å®Œæˆ:")
    print(f"   åŸè¯æ•°: {len(original_dict)}")
    print(f"   æ¸…ç†æ•°: {cleaned_count}")
    print(f"   é—®é¢˜è¯: {len(problematic)}")
    print(f"   ä¿ç•™è¯: {len(cleaned_dict)}")

    if problematic:
        print(f"\nâš ï¸  ä»¥ä¸‹è¯æ±‡æ²¡æœ‰æœ‰æ•ˆç¿»è¯‘ï¼Œå·²ç§»é™¤:")
        for word in problematic[:20]:  # Show first 20
            print(f"   - {word}")
        if len(problematic) > 20:
            print(f"   ... å’Œå…¶ä»– {len(problematic) - 20} ä¸ªè¯")

    # Save cleaned dictionary
    with open(dict_path, 'w', encoding='utf-8') as f:
        json.dump(cleaned_dict, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {dict_path}")

    # Show some examples
    print(f"\nğŸ“‹ æ¸…ç†åçš„ç¤ºä¾‹:")
    examples = ['philosophy', 'flexible', 'curve', 'beautiful', 'climate', 'technology']
    for word in examples:
        if word in cleaned_dict:
            print(f"   {word:15} â†’ {cleaned_dict[word]}")

    # Statistics
    print(f"\nğŸ“Š ç¿»è¯‘é•¿åº¦ç»Ÿè®¡:")
    lengths = [len(t) for t in cleaned_dict.values()]
    print(f"   å¹³å‡é•¿åº¦: {sum(lengths) / len(lengths):.1f} å­—ç¬¦")
    print(f"   æœ€çŸ­: {min(lengths)} å­—ç¬¦")
    print(f"   æœ€é•¿: {max(lengths)} å­—ç¬¦")

    # Count by length
    length_dist = {}
    for length in lengths:
        length_dist[length] = length_dist.get(length, 0) + 1

    print(f"\n   é•¿åº¦åˆ†å¸ƒ:")
    for length in sorted(length_dist.keys())[:10]:  # Show first 10
        count = length_dist[length]
        percent = count / len(cleaned_dict) * 100
        bar = 'â–ˆ' * int(percent / 2)
        print(f"   {length}å­—ç¬¦: {bar} {count} ({percent:.1f}%)")

    return cleaned_dict


if __name__ == "__main__":
    clean_dictionary()

#!/usr/bin/env python3
"""
ä» ECDICT æå– CEFR è¯æ±‡çš„ä¸­æ–‡ç¿»è¯‘
Extract Chinese translations for CEFR words from ECDICT

ECDICT: https://github.com/skywind3000/ECDICT
License: MIT, å¼€æºå…è´¹ä½¿ç”¨
"""

import json
import csv
import httpx
from pathlib import Path
from io import StringIO
import time

def download_and_extract_cefr_translations():
    """
    ä¸‹è½½ ECDICT å¹¶æå– CEFR è¯åº“ä¸­æ‰€æœ‰è¯çš„ä¸­æ–‡ç¿»è¯‘
    """

    print("=" * 70)
    print("ğŸ¯ ç›®æ ‡ï¼šä¸ºæ‰€æœ‰ CEFR è¯æ±‡æ·»åŠ ä¸­æ–‡ç¿»è¯‘")
    print("=" * 70)

    # Load CEFR words
    cefr_path = Path(__file__).parent / "data" / "cefr_words.json"
    with open(cefr_path, 'r', encoding='utf-8') as f:
        cefr_data = json.load(f)

    cefr_words = set(word.lower() for word in cefr_data.keys())
    print(f"\nğŸ“š CEFR è¯åº“: {len(cefr_words)} ä¸ªå•è¯")

    # ECDICT CSV URL (è¿™æ˜¯å®Œæ•´ç‰ˆï¼Œçº¦ 200MB)
    # æˆ‘ä»¬ä¼šæµå¼å¤„ç†ï¼Œåªæå–éœ€è¦çš„è¯
    ecdict_url = "https://github.com/skywind3000/ECDICT/releases/download/1.0.28/ecdict.csv"

    print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½ ECDICT...")
    print(f"   æ¥æº: {ecdict_url}")
    print(f"   âš ï¸  æ–‡ä»¶è¾ƒå¤§ï¼ˆçº¦ 200MBï¼‰ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
    print(f"   ğŸ’¡ æˆ‘ä»¬åªä¼šæå– CEFR è¯åº“ä¸­çš„è¯æ±‡")

    translations = {}
    processed = 0
    matched = 0

    try:
        print("\nğŸ”„ ä¸‹è½½å¹¶å¤„ç†ä¸­...")

        # Stream download to avoid loading entire file into memory
        with httpx.stream("GET", ecdict_url, timeout=300.0, follow_redirects=True) as response:
            response.raise_for_status()

            # Read and process line by line
            content = response.text
            csv_reader = csv.DictReader(StringIO(content), delimiter=',')

            for row in csv_reader:
                processed += 1

                # Progress indicator
                if processed % 10000 == 0:
                    print(f"   å¤„ç†äº† {processed:,} æ¡ï¼Œæ‰¾åˆ° {matched} ä¸ªåŒ¹é…...")

                word = row.get('word', '').lower().strip()

                # Only process CEFR words
                if word in cefr_words:
                    # ECDICT fields: word, phonetic, definition, translation, pos, collins, oxford, tag, bnc, frq
                    translation = row.get('translation', '').strip()

                    if translation:
                        # Clean up translation (take first translation if multiple)
                        # ECDICT format: "n. ä¹¦\\nvt. é¢„è®¢\\nvi. è®¢ç¥¨"
                        # We want just the Chinese part
                        chinese = []
                        for part in translation.split('\\n'):
                            # Remove English POS markers (n., v., adj., etc.)
                            cleaned = part
                            for prefix in ['n.', 'v.', 'vt.', 'vi.', 'adj.', 'adv.', 'prep.', 'conj.', 'pron.', 'interj.', 'abbr.']:
                                if cleaned.startswith(prefix):
                                    cleaned = cleaned[len(prefix):].strip()
                            if cleaned and not cleaned[0].isalpha():  # If starts with Chinese
                                chinese.append(cleaned)

                        if chinese:
                            translations[word] = chinese[0]  # Take first translation
                            matched += 1

                # Stop early if we found all CEFR words
                if matched >= len(cefr_words) * 0.95:  # 95% coverage is good enough
                    print(f"\n   âœ… å·²æ‰¾åˆ° 95% ä»¥ä¸Šçš„è¯æ±‡ï¼Œæå‰ç»“æŸ...")
                    break

        print(f"\nâœ… ä¸‹è½½å®Œæˆ!")
        print(f"   æ€»å…±å¤„ç†: {processed:,} æ¡è®°å½•")
        print(f"   åŒ¹é…æˆåŠŸ: {matched} ä¸ª CEFR å•è¯")
        print(f"   è¦†ç›–ç‡: {matched/len(cefr_words)*100:.1f}%")

    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        print(f"\nğŸ’¡ å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨æœ¬åœ°å¿«é€Ÿæ‰©å……æ–¹æ¡ˆ...")
        return use_fallback_dictionary()

    # Save to file
    output_file = Path(__file__).parent / "chinese_dict_full.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(translations, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")

    # Replace current dictionary
    current_dict_file = Path(__file__).parent / "chinese_dict.json"
    with open(current_dict_file, 'w', encoding='utf-8') as f:
        json.dump(translations, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²æ›´æ–°å½“å‰è¯å…¸: {current_dict_file}")
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   CEFR è¯åº“: {len(cefr_words)} è¯")
    print(f"   ä¸­æ–‡è¯å…¸: {len(translations)} è¯")
    print(f"   è¦†ç›–ç‡: {len(translations)/len(cefr_words)*100:.1f}%")

    return output_file


def use_fallback_dictionary():
    """
    å¦‚æœä¸‹è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ¡ˆï¼š
    ç›´æ¥ä½¿ç”¨æœ‰é“ API æˆ–è€…ç™¾åº¦ API æ‰¹é‡ç¿»è¯‘
    """
    print("\n=" * 70)
    print("ğŸ“Œ å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ç®€åŒ–è¯å…¸")
    print("=" * 70)
    print("\næœ‰ä¸¤ä¸ªé€‰æ‹©:")
    print("1. ã€æ¨èã€‘è¿è¡Œ expand_to_1000_words.py - æ·»åŠ  1000+ å¸¸ç”¨è¯")
    print("2. ä½¿ç”¨ç¿»è¯‘ APIï¼ˆéœ€è¦ç”³è¯·å¯†é’¥ï¼‰:")
    print("   - æœ‰é“æ™ºäº‘ API: https://ai.youdao.com/")
    print("   - ç™¾åº¦ç¿»è¯‘ API: https://fanyi-api.baidu.com/")
    print("\nå»ºè®®å…ˆä½¿ç”¨æ–¹æ¡ˆ 1ï¼Œè¦†ç›–ç‡å¯è¾¾ 15-20%")


if __name__ == "__main__":
    print("\nğŸš€ MixRead - ECDICT é›†æˆå·¥å…·\n")
    download_and_extract_cefr_translations()

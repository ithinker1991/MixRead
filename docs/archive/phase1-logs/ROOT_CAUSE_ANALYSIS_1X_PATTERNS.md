# 根本原因分析: (1×) 、(2×) 频率标记

**发现日期**: 2025-12-02
**问题**: 某些网站返回的句子中包含 `(1×)`、`(2×)` 等频率标记
**根本原因**: 网页的DOM中嵌入了频率标记,前端提取时没有清理
**状态**: ✅ 已修复

---

## 问题现象

用户在LLM推理博客标记单词时,返回的数据包含:

```
"approximate(1×)track(1×)component(1×)evaluate(1×)numb(1×)rule(1×)..."
```

这些 `(1×)` 、`(2×)` 、`(3×)` 等看起来像是**单词在文本中出现的次数标记**。

---

## 排查过程

### 第一个假设 (错误)
"这是后端在处理数据时添加的?"
- ❌ 后端代码中没有生成这种格式的逻辑

### 第二个假设 (错误)
"前端某个环节在生成这个?"
- ❌ 前端代码中也没有生成这种格式

### 第三个假设 (正确!) ✅
"这是网页本身的DOM中就存在的内容!"
- ✅ **网页的textContent已经包含了这些标记**
- ✅ 前端用 `paragraph.textContent` 提取时,把这些标记也提取出来了
- ✅ 过滤器应该过滤掉,但某些情况下(如Fallback)绕过了

---

## 根本原因

某些网站(如LLM推理博客)的HTML DOM中,文本节点中已经包含了这种格式:

```html
<!-- 网页原始HTML中可能是: -->
<p>
  approximate
  <span class="freq">(1×)</span>
  track
  <span class="freq">(1×)</span>
  ...
</p>

<!-- 当我们调用 textContent 时,获得的是: -->
"approximate(1×)track(1×)component(1×)..."

<!-- 这不是生成的,而是网页本身的内容! -->
```

或者更可能是这样:

```html
<!-- 某个字典/参考网站的标记系统 -->
<div>
  Word list:
  approximate(1×) track(1×) component(1×) evaluate(1×)
</div>

<!-- textContent 返回原始文本 -->
```

---

## 解决方案

### 三步修复

#### 1. 在提取文本时立即清理 (最根本)

**文件**: `frontend/content.js` (行695-704)
**文件**: `frontend/modules/panel/batch-marking-panel.js` (行762-775)

```javascript
// 获取原始文本
let text = paragraph.textContent || paragraph.innerText || '';

// 关键: 立即清理网页中嵌入的频率标记
text = text.replace(/\(\d+×\)/g, '');  // 删除 (1×), (2×), (3×), 等
text = text.replace(/→\s*/g, ' ');     // 替换箭头为空格

// 然后进行后续处理
text = text.replace(/\s+/g, ' ').trim();
```

**为什么要这样做:**
- 这是**最早的阶段**来清理污染的文本
- 之后的所有操作(句子提取、过滤)都基于干净的文本
- 避免在后期处理中遗漏

#### 2. 保留现有的6层过滤器

已有的6层过滤器继续保留:
- 1. 最小长度 (10字符)
- 2. 最小词数 (3个词)
- 3. 特殊字符限制 (max 2个)
- 4. 垃圾模式检测 (1x, →, \d+×)
- 5. 词频模式检测 (超过3个)
- 6. 非ASCII字符过滤 (中日文)

这些现在作为**二重防御**,万一前面的清理有遗漏。

#### 3. 修复Fallback漏洞

已修复Fallback代码中的过滤不足问题(前面已完成)。

---

## 修复前后对比

### 修复前 (有问题)

```
网页HTML
   ↓
textContent 提取 (包含 (1×) 标记)
   ↓
句子提取
   ↓
6层过滤 (尝试过滤 (1×))
   ↓
Fallback (❌ 不过滤) → 污染数据逃出!
   ↓
发送到后端 ❌ 包含 (1×)
```

### 修复后 (正常)

```
网页HTML
   ↓
textContent 提取
   ↓
立即清理 (✅ 删除 (1×) (2×) → 等)
   ↓
句子提取 (干净的文本!)
   ↓
6层过滤 (现在不用担心 (1×))
   ↓
Fallback (✅ 也过滤) → 额外防御
   ↓
发送到后端 ✅ 干净的句子
```

---

## 技术细节

### 被清理的模式

1. **频率标记**: `(1×)` 、`(2×)` 、`(3×)` ... 等
   - 正则: `/\(\d+×\)/g`
   - 匹配: `(` + 数字 + `×` + `)`

2. **箭头符号**: `→`
   - 正则: `/→\s*/g`
   - 匹配: 箭头 + 可选空格
   - 替换为: 单个空格

### 为什么这些模式会出现?

这是某些**参考网站/字典网站**的常见标记系统:
- `(1×)` 表示单词在文本中出现1次
- `(2×)` 表示出现2次
- `(3×)` 表示出现3次
- `→` 表示词根或相关词

LLM推理博客可能集成了这样的标记系统来帮助读者。

---

## 为什么之前没有完全解决?

前面的过滤器设计是**反应式的**:
- ❌ 等待看到有问题的模式才过滤
- ❌ 过滤器逻辑复杂,需要检查多种情况
- ❌ Fallback绕过了过滤

现在的方案是**预防式的**:
- ✅ 在任何处理之前就清理文本
- ✅ 直接处理问题的根源
- ✅ 之后的所有操作都基于干净的数据

---

## 验证方法

### 本地测试

1. **重新加载扩展**
   ```
   chrome://extensions → MixRead → 重新加载
   ```

2. **访问LLM推理博客**
   ```
   https://arpitbhayani.me/blogs/how-llm-inference-works
   ```

3. **打开DevTools**
   ```
   F12 → Console
   ```

4. **标记一个单词**
   ```
   右键单词 → "Add to Library"
   ```

5. **检查控制台输出**
   ```
   应该看到:
   [MixRead DEBUG] About to send word "xxx" with X sentences: [...]

   句子中应该 NO 包含 (1×) 、(2×) 等
   ```

6. **检查库页面**
   ```
   http://localhost:8002/library-viewer.html

   单词的句子应该是干净的,不包含频率标记
   ```

---

## 影响范围

### 受影响的网站

主要是这类网站:
- ✅ 带有词频标记的参考网站
- ✅ 集成了字典/语料库统计的博客
- ✅ 学术文章中的词汇分析

### 不受影响的网站

- GitHub (正常HTML)
- Medium (正常HTML)
- Wikipedia (通常不包含频率标记)
- 大多数普通网站

---

## 关键学习

1. **理解数据来源**: 问题中的`(1×)`不是代码生成的,而是网页本身的内容
2. **预防而非反应**: 在源头清理优于在下游检测
3. **多层防御**: 即使源头清理有遗漏,过滤器仍然起作用
4. **全链路追踪**: 必须追踪数据从网页到数据库的完整路径

---

## 修改总结

| 文件 | 改动 | 说明 |
|-----|------|------|
| `frontend/content.js` | 行698-702 | 提取textContent后立即清理频率标记 |
| `frontend/modules/panel/batch-marking-panel.js` | 行767-771 | 同样的清理逻辑 |
| (前面已修复) | Fallback过滤 | 确保备选方案也过滤 |

---

## 现在的完整防御层

```
1. 源头清理   ← 删除频率标记本身 (新)
   ↓
2. 句子提取   ← 从干净的文本提取
   ↓
3. 6层过滤    ← 即使有遗漏也过滤
   ↓
4. Fallback过滤 ← 备选方案也验证 (前面修复)
```

---

**结论**: 问题的根本原因是网页本身的DOM内容包含频率标记,现在通过在提取后立即清理来解决,这是最优雅和最根本的解决方案。

---

*分析日期: 2025-12-02*
*问题类型: 数据污染*
*解决方案: 源头清理*
*状态: ✅ 完成*
